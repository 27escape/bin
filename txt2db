#!/usr/bin/env perl
# convert a csv like text file into a series of db statements
# this can be used to either manage proper csv files
# or unstructured text files that people want in an database
#
# (c) kevin Mulholland 2004, moodfarm@cpan.org
# this code is released under the Perl Artistic License

# v0.2 4/11/2009 moodfarm@cpan.org, variation on my txt2xls script
# v0.3 28/09/2011 moodfarm@cpan.org, using Text::CSV::Slurp 
# v0.4 14/02/2012 moodfarm@cpan.org, fixed index issue when csv header not uppercase
# v0.5 09/07/2013 moodfarm@cpan.org, added field indexing, refactored to stream using
#   Text::CSV::Auto to save memory when processing massive (> 2GB) files
# v0.6 31/08/2013 moodfarm@cpan.org, replaced Text::CSV::Auto with Text::CSV_XS
#   as the former cannot handle STDIN, new process seems faster too!

use 5.16.0;
use feature "state";
use strict;
use warnings;
use utf8::all;
use Text::CSV_XS;
use Try::Tiny;
use Encode;
use Clone qw(clone);
use Data::Printer;

use App::Basis;

# -----------------------------------------------------------------------------

use constant BUFFER_SIZE => 20_000_000;

my $DELIMITOR     = ',';
my $ANALYSE_LINES = 5000;

# -----------------------------------------------------------------------------
# stolen from neech and modified
sub looks_like {
    local $_ = shift;
    my $lastGuess = shift || [ ( 'NOTSEEN', 0 ) ];
    my $maxLen = length($_) > $lastGuess->[1] ? length($_) : $lastGuess->[1];
    my %colWeight = (
        NOTSEEN     => 0,
        BOOL        => 1,
        INT         => 2,
        FLOAT       => 3,
        DATETIME    => 4,
        DATE        => 5,
        TIME        => 6,
        IP          => 7,
        MAC_ADDRESS => 8,
        VARCHAR     => 9,
        TEXT        => 10
    );

    return $lastGuess if !defined $_ || length($_) < 1;
    return [ ( 'BOOL', $maxLen ) ] if $colWeight{ $lastGuess->[0] } <= $colWeight{BOOL} && ( !defined $_ || $_ eq '' || /^[01]$/ );
    return [ ( 'INT', $maxLen ) ] if $colWeight{ $lastGuess->[0] } <= $colWeight{INT} && /^-?[0-9,]+$/;
    return [ ( 'FLOAT', $maxLen ) ] if $colWeight{ $lastGuess->[0] } <= $colWeight{FLOAT} && /^-?[0-9,]*\.[0-9]+$/;

    return [ ( 'DATETIME', $maxLen ) ]
        if $colWeight{ $lastGuess->[0] } <= $colWeight{DATETIME}
        && ( m|^\d{1,2}/\d{1,2}/\d{2,4}[\s.]\d{1,2}:\d{1,2}:\d{1,2}\s[AP]M$|i || /^\d{2,4}-\d{1,2}-\d{1,2}[\s\.T]\d{1,2}:\d{1,2}:\d{1,2}(\+\d{4})?/ );
    return [ ( 'DATE', $maxLen ) ] if $colWeight{ $lastGuess->[0] } <= $colWeight{DATE} && ( m|^\d{1,2}/\d{1,2}/\d{2,4}$| || /^\d{2,4}-\d{1,2}-\d{1,2}$/ );
    return [ ( 'TIME', $maxLen ) ] if $colWeight{ $lastGuess->[0] } <= $colWeight{TIME} && /^\d{1,2}:\d{1,2}:\d{1,2}$/;

    return [ ( 'IP',          $maxLen ) ] if $colWeight{ $lastGuess->[0] } <= $colWeight{IP}          && /^(?:\d{1,3}\.){3}\d{1,3}$/;
    return [ ( 'MAC_ADDRESS', $maxLen ) ] if $colWeight{ $lastGuess->[0] } <= $colWeight{MAC_ADDRESS} && /^(?:[[:xdigit:]]{2}[-:]){5}[[:xdigit:]]{2}$/;

    return [ ( 'VARCHAR', $maxLen ) ] if $colWeight{ $lastGuess->[0] } <= $colWeight{VARCHAR} && length($_) <= 255;
    return [ ( 'TEXT', $maxLen ) ];
}

# -----------------------------------------------------------------------------
# stolen from neech, replaced $_ with $v to stop it messing with the
# syntax highlighter!
sub round_up {
    my $v = shift || 0;
    $v += 10 if $v >= 10;    # A little extra space
    my @snap  = qw(1 4 8 16 32 48 64);
    my $multi = 10;

    my $sret;
    for my $snap (@snap) {
        if ( $snap >= $v ) {
            $sret = $snap;
            last;
        }
    }

    my $mret
        = $v % $multi
        ? ( ( int( $v / $multi ) * $multi ) + $multi )
        : $v;
    $mret ||= $multi;

    return defined $sret && $sret <= $mret ? $sret : $mret;
}

# -----------------------------------------------------------------------------
# consistent way to clean up field names, we store the pre and post versions
# to speed up repeated conversions
sub clean_field {
    state %clean_data;
    my $field = shift;
    my $pre   = $field;

    if ( !$clean_data{$field} ) {
        $field =~ s/[\s\(\{\.\:]/_/g;
        $field =~ s/[\#\;\\\|\/\)\}\'\"]//g;

        # index seems to be a special name esp for sqlite
        $field = '_index' if ( $field eq 'index' );
        $clean_data{$pre} = lc $field;
    }

    return $clean_data{$pre};
}

# -----------------------------------------------------------------------------
# stolen from neech and modified
sub generate_schema {
    my ( $table, $data, $primary_keys ) = @_;
    return unless defined $table && defined $data;

    my $schema = {};
    my $count  = scalar( @{$data} );
    my %keymap;

    for ( my $i = 0; $i < $count; $i++ ) {
        my $struct = @{$data}[$i];
        while ( my ( $k, $v ) = each %{$struct} ) {
            my $real_name = $k;

            # do some tidy ups of the field names
            my $clean = clean_field($k);
            if ( !$keymap{$real_name} ) {
                $keymap{$real_name} = $clean;
            }

            my $aref = looks_like( $v, $schema->{$clean} );
            $schema->{$clean} = $aref if ( $aref->[0] ne 'NOTSEEN' );
        }
    }

    # now we need to go through the schema and decide which columns we want to index on
    # int/float/date/datetime/time/ip/mac_address, for the latter 2, we also need then to
    # rename them to varchar fields
    # index lots of things, it cannot hurt!

    my $index_str = "";

    while ( my ( $field, $value ) = each %$schema ) {
        my ( $field_type, $size ) = @$value;
        if ( $field_type =~ /INT|FLOAT|DATE|TIME|IP|MAC/i ) {
            $field_type = lc($field_type);
            $index_str .= "CREATE INDEX IF NOT EXISTS $table" . "_$field" . "_$field_type" . "_idx on $table($field) ;\n";

            $schema->{$field} = [ 'VARCHAR', $size ] if ( $field_type =~ /ip|mac/ );
        }
    }

    my @columns       = ();
    my @columns_order = sort keys %{$schema};
    for my $column (@columns_order) {
        next if ( !$schema->{$column} );
        my ( $type, $length ) = @{ $schema->{$column} };
        push @columns, sprintf( "\t%s %s%s", lc($column), $type, ( grep( $_ eq $type, qw(VARCHAR INT) ) ? '(' . round_up($length) . ')' : '' ), );
    }

    my $primarykey = $primary_keys ? ",\n\tPRIMARY KEY ($primary_keys)" : '';

    my $sql = "CREATE TABLE IF NOT EXISTS $table (\n" . join( ",\n", @columns ) . $primarykey . "\n) ;\n";

    # add in the index
    $sql .= $index_str;

    # add the real field names into the schema
    foreach my $k ( keys %$schema ) {
        my $a   = $schema->{$k};
        my $val = $keymap{$k};
        if ( !$val ) {
            $val = $k eq '_index' ? 'index' : $k;
        }
        push @$a, $val;
        $schema->{$k} = $a;
    }

    return ( $schema, $sql );
}

# -----------------------------------------------------------------------------
# stolen from neech, and modified
# prints the table info as soon as it finds it, makes piping into other commands
# that bit faster
sub write_table {
    my ( $table, $dat, $drop, $replace, $primary_keys, $delete ) = @_;
    return unless defined $dat && ref($dat) eq 'ARRAY';

    $replace = $replace ? "OR REPLACE" : "";

    say encode( 'UTF-8', "DROP TABLE IF EXISTS '$table' ;" ) if ($drop);

    my ( $schema, $sql ) = generate_schema( $table, $dat, $primary_keys );
    say encode( 'UTF-8', $sql );
    say encode( 'UTF-8', "DELETE from '$table' ;" ) if ($delete);

    my @columns = sort keys %{$schema};

    foreach my $d ( @{$dat} ) {
        my $add = 0;

        # test if all empty, do not add it if so
        map { $add += ( defined $_ && $_ eq '' ) ? 0 : 1 } values %$d;
        if ($add) {

            # remember we need to escape single quotes with two single quotes
            say encode(
                'UTF-8',
                "INSERT $replace INTO '$table' (" . join( ', ', map {"'$_'"} @columns ) . ") VALUES (" . join(
                    ', ',
                    map {
                        my $key        = $schema->{$_}->[2];
                        my $field_type = $schema->{$_}->[0];
                        $key = $key eq '_index' ? 'index' : $key;
                        my $out = "";
                        if ( $key && defined $d->{$key} ) {
                            $out = $d->{$key};

                            # remove commas for numbers
                            $out =~ s/,//g if ( $field_type =~ /INT|FLOAT/ );

                            # we are guessing that dates are dd/mm/yyy and can be swapped around
                            # if they are mm/dd/yyyy then they will also be swapped and messed up
                            $out =~ s|^(\d{1,2})[/-](\d{1,2})[/-](\d{4})$|$3/$2/$1| if ( $field_type eq 'DATE' );

                            $out =~ s/'/''/g;
                            $out = "'$out'";
                        }
                        $out
                    } @columns
                    )
                    . ") ;"
            );
        }
    }

    return $schema;
}

# -----------------------------------------------------------------------------
sub write_line {
    my ( $table, $schema, $row, $replace ) = @_;
    $replace = $replace ? "OR REPLACE" : "";

    my @columns = sort keys %{$schema};

    # remember we need to escape single quotes with two single quotes
    say encode(
        'UTF-8',
        "INSERT $replace INTO '$table' (" . join( ', ', map {"'$_'"} @columns ) . ") VALUES (" . join(
            ', ',
            map {
                my $key = $schema->{$_}->[2];
                my $out = "";
                if ( $key && defined $row->{$key} ) {
                    $out = $row->{$key};

                    # remove commas for numbers
                    $out =~ s/,//g if ( $schema->{$key}->[0] && $schema->{$key}->[0] =~ /INT|FLOAT/ );
                    $out =~ s/'/''/g;
                    $out = "'$out'";
                }
                $out
            } @columns
            )
            . ") ;"
    );
}

# -----------------------------------------------------------------------------
sub process_csv {
    my ( $infilename, $table, $opt ) = @_;
    my $line      = 0;
    my $processed = 0;
    my ( $fh, $schema, $csv, @analyse_data );

    if ( $infilename eq '-' ) {

        # reopen/dup STDIN
        if ( !open( $fh, "<&", \*STDIN ) ) {
            say STDERR "Failed to dup STDIN";
            return 0;
        }
    }
    else {
        if ( !open( $fh, "<", $infilename ) ) {
            say STDERR "Failed to open $infilename";
            return 0;
        }
    }

    # for sqlite we can disable journalling for speedier inserts, riskier though
    say encode(
        'UTF-8', "
PRAGMA synchronous = 0 ;
-- PRAGMA journal_mode = off ;
PRAGMA cache_size=8192 ;
-- start the transaction
BEGIN ;"
    );

    $csv = Text::CSV_XS->new( { sep_char => $opt->{delimitor}, binary => 1, allow_loose_escapes => 1, auto_diag => 0 } );

    my @headers;

    # get the first line and calc the fieldnames, make them nice
    my $header_line = <$fh>;
    my $split       = $opt->{delimitor};
    $split = '\|' if ( $split eq '|' );
    foreach my $field ( split( $split, $header_line ) ) {
        $field =~ s/^\s?^"(.*?)"\s?$/$1/;    # remove any leading/trailing space and double quotes
        $field =~ s/^\s?^(.*?)\s?$/$1/;      # remove any leading/trailing space
        $field =~ s/\s/_/g;                  # replace spaces
        $field =~ s/^#//;                    # replace leading hash its not good
        $field = lc($field);
        push @headers, $field;
    }
    msg_exit( "This does not appear to be CSV data", 2 ) if ( !@headers );

    # repopulate $row on each getline
    while ( my $trow = $csv->getline($fh) ) {
        my $row;
        map { $row->{$_} = shift @$trow; } @headers;

        if ( $line++ < $opt->{lines} ) {

            # we need to clone the data as $row reference is updated each getline
            push @analyse_data, clone $row ;
        }
        else {
            if ( !$processed ) {
                $processed = 1;

                # say STDERR "a. " . p(@analyse_data) ;
                $schema = write_table( $table, \@analyse_data, $opt->{create}, $opt->{replace}, $opt->{key} );

                # make sure schema and indexing done
                say encode( 'UTF-8', "COMMIT ;" );
            }
            else {
                if ( $processed == 1 ) {
                    say encode( 'UTF-8', "BEGIN ;" );
                    $processed = 2;
                }
                write_line( $table, $schema, $row, $opt->{replace} );
            }
        }
    }
    if ( !$processed ) {

        # say STDERR "a. " . p(@analyse_data) ;
        write_table( $table, \@analyse_data, $opt->{create}, $opt->{replace}, $opt->{key}, $opt->{delete} );
    }
    say encode( 'UTF-8', "COMMIT ;" );

    return $line;
}

# -----------------------------------------------------------------------------
# main
my $program = get_program();
my %opt     = init_app(
    help_text    => "Convert CSV data into SQL insert/replace statements",
    help_cmdline => "[tablename,]inputfilename ...
    $program -            (read from stdin)
    $program filename.csv
    $program db_table,csv1.csv
    $program table1,csv1.csv table2,csv2.csv ... tablex,csvx.csv
    $program --delimitor=';' filename.csv
",
    options => {
        'delimitor=s' => { desc => "delimitor/separator of columns", default => $DELIMITOR },
        'lines|l=i'   => {
            desc     => "number of lines lines to analyse CSV fields (min 10)",
            default  => $ANALYSE_LINES,
            validate => sub { my $val = shift; return $val >= 10; }
        },
        'create|c'  => 'create a new table called tablename',
        'delete|d'  => 'delete all table entries before insert',
        'replace|r' => 'create replace rather than insert SQL statements',
        'add|a'     => 'Add extra field with value to every CSV row',

        # 'create_id'   => 'create an autoincrementing row id field',
        'key|k=s' => 'comma separated list of heads that should become primary keys',
    }
);

show_usage("Bad arguments") if ( scalar @ARGV < 1 );

# if we want the pipe symbol, make sure its correct for regexp
$opt{delimitor} = '|'  if ( $opt{delimitor} eq '|' );
$opt{delimitor} = "\t" if ( $opt{delimitor} eq '\t' );

my $count = 1;

# now process the files listed on the command line
foreach my $name (@ARGV) {
    my $infilename;
    my $table;

    # either filename or "table,filename"
    my ( $a, $b ) = split( /,/, $name );

    # if no table then add a default
    if ( !$b ) {
        $infilename = $a;

        # use the filename up to any '.' as the table name
        $a =~ m/(\w+)\.\w+$/;

        # name the table
        $table = $1 ? $1 : "table_$count";
        $count++;
    }
    else {
        $infilename = $b;
        $table      = $a;
    }

    # make sure the table name is ok
    $table      =~ s/^\s+(.*?)\s+$/$1/;    # trim leading/trailing whitespace
    $table      =~ s/[- ]/_/g;             # replace minus with underscore
    $table      =~ s/["']//g;              # remove double quotes
    $infilename =~ s/^~/$ENV{HOME}/;
    my $status = process_csv( $infilename, $table, \%opt );
    if ($status) {
        if ( $opt{verbose} ) {
            say "processed " . ( $infilename ne '-' ? $infilename : "" );
        }
    }
    else {
        say STDERR "failed to process $infilename";
    }
}
